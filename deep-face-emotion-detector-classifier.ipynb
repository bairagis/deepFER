{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a391ba42",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0647497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 397, in dispatch_shell\n",
      "    await result\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 752, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_11511/3252540011.py\", line 6, in <module>\n",
      "    import albumentations as A\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/albumentations/__init__.py\", line 24, in <module>\n",
      "    from .pytorch import *\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/albumentations/pytorch/__init__.py\", line 1, in <module>\n",
      "    from .transforms import *\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/albumentations/pytorch/transforms.py\", line 15, in <module>\n",
      "    import torch\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import albumentations as A\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "092a46eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of images in a directory\n",
    "def count_images_in_directory(directory):\n",
    "    count = 0\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# find leaf directories in a given directory\n",
    "def find_leaf_directories(directory):\n",
    "    leaf_dirs = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        if not dirs:  # if there are no subdirectories\n",
    "            leaf_dirs.append(root)\n",
    "    return leaf_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b880df84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dataset/images/train/sad',\n",
       " 'Dataset/images/train/fear',\n",
       " 'Dataset/images/train/surprise',\n",
       " 'Dataset/images/train/neutral',\n",
       " 'Dataset/images/train/disgust',\n",
       " 'Dataset/images/train/happy',\n",
       " 'Dataset/images/train/angry']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaf_dirs_images_images= find_leaf_directories('Dataset/images/train')\n",
    "leaf_dirs_images_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c835ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: Dataset/images/train/sad, Number of images: 4938\n",
      "Directory: Dataset/images/train/fear, Number of images: 4103\n",
      "Directory: Dataset/images/train/surprise, Number of images: 3205\n",
      "Directory: Dataset/images/train/neutral, Number of images: 4982\n",
      "Directory: Dataset/images/train/disgust, Number of images: 436\n",
      "Directory: Dataset/images/train/happy, Number of images: 7164\n",
      "Directory: Dataset/images/train/angry, Number of images: 3993\n"
     ]
    }
   ],
   "source": [
    "for leaf_dir in leaf_dirs_images_images:\n",
    "    image_count = count_images_in_directory(leaf_dir)\n",
    "    print(f\"Directory: {leaf_dir}, Number of images: {image_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8446a151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dataset/images/validation/sad',\n",
       " 'Dataset/images/validation/fear',\n",
       " 'Dataset/images/validation/surprise',\n",
       " 'Dataset/images/validation/neutral',\n",
       " 'Dataset/images/validation/disgust',\n",
       " 'Dataset/images/validation/happy',\n",
       " 'Dataset/images/validation/angry']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaf_dirs_images_train = find_leaf_directories('Dataset/images/validation')\n",
    "leaf_dirs_images_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43e39699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: Dataset/images/validation/sad, Number of images: 1139\n",
      "Directory: Dataset/images/validation/fear, Number of images: 1018\n",
      "Directory: Dataset/images/validation/surprise, Number of images: 797\n",
      "Directory: Dataset/images/validation/neutral, Number of images: 1216\n",
      "Directory: Dataset/images/validation/disgust, Number of images: 111\n",
      "Directory: Dataset/images/validation/happy, Number of images: 1825\n",
      "Directory: Dataset/images/validation/angry, Number of images: 960\n"
     ]
    }
   ],
   "source": [
    "for leaf_dir in leaf_dirs_images_train:\n",
    "    image_count = count_images_in_directory(leaf_dir)\n",
    "    print(f\"Directory: {leaf_dir}, Number of images: {image_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d347a263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(os.listdir('Dataset/images/train'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5cf4917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image sizes in the directory: {(48, 48)}\n"
     ]
    }
   ],
   "source": [
    "# find out the pixel size of the images in the directory\n",
    "def find_image_size(directory):\n",
    "    sizes = set()\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "            img_path = os.path.join(directory, filename)\n",
    "            img = Image.open(img_path)\n",
    "            sizes.add(img.size)\n",
    "    return sizes\n",
    "\n",
    "image_sizes = find_image_size('Dataset/images/train/fear')\n",
    "print(f\"Image sizes in the directory: {image_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43793102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-05 12:15:55.356901: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754376355.370895   11511 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754376355.375192   11511 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754376355.387420   11511 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754376355.387440   11511 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754376355.387442   11511 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754376355.387443   11511 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-05 12:15:55.391203: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from PIL import Image\n",
    "from deepface import DeepFace\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08547a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, root_dir,embedder, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        for idx, cls in enumerate(self.classes):\n",
    "            cls_dir = os.path.join(root_dir, cls)\n",
    "            for fname in os.listdir(cls_dir):\n",
    "                if fname.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                    self.image_paths.append(os.path.join(cls_dir, fname))\n",
    "                    self.labels.append(idx)\n",
    "       \n",
    "        self.embedder = embedder  # Keras model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Add this in a TRy Except block to handle errors\n",
    "\n",
    "        try:\n",
    "            img_path = self.image_paths[idx]\n",
    "            label = self.labels[idx]\n",
    "            print(f\"Processing image {img_path} with label {label}\")\n",
    "            # Load grayscale then convert to RGB\n",
    "            img = Image.open(img_path).convert('L').convert('RGB')\n",
    "\n",
    "            # covert this to a numpy array\n",
    "            img = np.array(img)\n",
    "            \n",
    "            representations = DeepFace.represent(\n",
    "                img_path=img,\n",
    "                model_name='VGG-Face',\n",
    "                detector_backend='mtcnn'  # A robust face detector\n",
    "                )\n",
    "\n",
    "            # The result is a list of dictionaries, one for each face detected in the image.\n",
    "            # We'll assume there is only one face for this example.\n",
    "            if representations:\n",
    "                embedding = representations[0]['embedding']\n",
    "                \n",
    "            # Convert to torch tensor\n",
    "            embedding = torch.tensor(embedding, dtype=torch.float)\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_path}: {e}\")\n",
    "            embedding = torch.zeros(4096, dtype=torch.float)  # Assuming 2622 is the embedding size\n",
    "            label = -1  # Use -1 or some other value to indicate an error\n",
    "        \n",
    "        print(f\"Processed image {img_path} with label {label}\")\n",
    "        return embedding, label\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3247adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 397, in dispatch_shell\n",
      "    await result\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 752, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_3346/1208061498.py\", line 1, in <module>\n",
      "    import torch.nn as nn\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, embedding_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_size, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ae847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_dir,\n",
    "                num_epochs=3,\n",
    "                batch_size=500,\n",
    "                lr=1e-3,\n",
    "                save_path='emotion_detector_deepface.pth'):\n",
    "    # MTCNN for face detection\n",
    "    # mtcnn = MTCNN(image_size=160, margin=0)\n",
    "    # Load DeepFace embedding model\n",
    "    print(\"Loading DeepFace embedding model...\")\n",
    "\n",
    "    embedder = DeepFace.build_model('VGG-Face') \n",
    "    # Dataset + DataLoader\n",
    "    dataset = EmotionDataset(root_dir=data_dir,\n",
    "                            #  mtcnn=mtcnn,\n",
    "                             embedder=embedder)\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=5)\n",
    "    \n",
    "    print(f\"Dataset loaded with {len(dataset)} samples.\")\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = EmotionClassifier(embedding_size=4096,  # VGG-Face embedding size\n",
    "                              num_classes=len(dataset.classes))\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for embeddings, labels in dataloader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(embeddings)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * embeddings.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = correct / total\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Acc: {epoch_acc:.4f}')\n",
    "\n",
    "    # Save classifier state and classes\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'class_names': dataset.classes\n",
    "    }, save_path)\n",
    "    print(f'Model saved to {save_path}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "084a6234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1754376358.798349   11511 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m lr = \u001b[32m1e-3\u001b[39m   \n\u001b[32m      6\u001b[39m save_path = \u001b[33m'\u001b[39m\u001b[33memotion_detector_deepface.pth\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_path\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(data_dir, num_epochs, batch_size, lr, save_path)\u001b[39m\n\u001b[32m     29\u001b[39m correct = \u001b[32m0\u001b[39m\n\u001b[32m     30\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# embeddings = embeddings.to(device)\u001b[39;49;00m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# labels = labels.to(device)\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    635\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1329\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1328\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1332\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1295\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1291\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1292\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1293\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1294\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1296\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1297\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/sudip/linux-extra/code/almabetter/deepFER/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1133\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1121\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1122\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1130\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1131\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1132\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1133\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1134\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1135\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1136\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1137\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/multiprocessing/connection.py:261\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    260\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/multiprocessing/connection.py:428\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/multiprocessing/connection.py:935\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m    932\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m    934\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m935\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    936\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m    937\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "   \n",
    "data_dir = 'Dataset/images/train'\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "lr = 1e-3   \n",
    "save_path = 'emotion_detector_deepface.pth'\n",
    "\n",
    "\n",
    "train_model(\n",
    "        data_dir=data_dir,\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        lr=lr,\n",
    "        save_path=save_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d0a5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9186e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in the dataset: {0, 1, 2, 3, 4, 5, 6}\n"
     ]
    }
   ],
   "source": [
    "dataset = EmotionDataset(\n",
    "    root_dir='Dataset/images/train',\n",
    "    embedder=DeepFace.build_model('VGG-Face')\n",
    ")\n",
    "\n",
    "# find the set of labels in the dataset\n",
    "labels = set(dataset.labels)\n",
    "print(f\"Unique labels in the dataset: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "570648b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (4096,)\n",
      "Embedding (first 10 values): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Image path: [[[254 254 254]\n",
      "  [244 244 244]\n",
      "  [174 174 174]\n",
      "  ...\n",
      "  [140 140 140]\n",
      "  [235 235 235]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[254 254 254]\n",
      "  [232 232 232]\n",
      "  [136 136 136]\n",
      "  ...\n",
      "  [124 124 124]\n",
      "  [185 185 185]\n",
      "  [250 250 250]]\n",
      "\n",
      " [[254 254 254]\n",
      "  [220 220 220]\n",
      "  [100 100 100]\n",
      "  ...\n",
      "  [124 124 124]\n",
      "  [136 136 136]\n",
      "  [218 218 218]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 43  43  43]\n",
      "  [ 42  42  42]\n",
      "  [ 42  42  42]\n",
      "  ...\n",
      "  [ 38  38  38]\n",
      "  [ 47  47  47]\n",
      "  [ 49  49  49]]\n",
      "\n",
      " [[ 42  42  42]\n",
      "  [ 42  42  42]\n",
      "  [ 38  38  38]\n",
      "  ...\n",
      "  [ 41  41  41]\n",
      "  [ 43  43  43]\n",
      "  [ 37  37  37]]\n",
      "\n",
      " [[ 37  37  37]\n",
      "  [ 41  41  41]\n",
      "  [ 36  36  36]\n",
      "  ...\n",
      "  [ 32  32  32]\n",
      "  [ 37  37  37]\n",
      "  [ 34  34  34]]]\n",
      "Embedding size: 4096\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "import cv2\n",
    "\n",
    "# Load an image using OpenCV\n",
    "# You can replace 'path/to/your/image.jpg' with the actual path to your image\n",
    "# img = cv2.imread('Dataset/images/train/fear/35720.jpg')  # Example image path\n",
    "img = cv2.imread('Dataset/images/train/neutral/32462.jpg') \n",
    "\n",
    "# Check if the image was loaded successfully\n",
    "if img is None:\n",
    "    print(\"Error: Image not found or could not be loaded.\")\n",
    "else:\n",
    "    # Get the embedding using DeepFace.represent()\n",
    "    # The 'model_name' parameter specifies which model to use.\n",
    "    # 'VGG-Face' is the default and a good choice.\n",
    "    # The 'detector_backend' parameter can be changed if you want a different face detector.\n",
    "    # 'opencv', 'ssd', 'dlib', 'mtcnn', 'retinaface', 'mediapipe' are available.\n",
    "    representations = DeepFace.represent(\n",
    "        img_path=img,\n",
    "        model_name='VGG-Face',\n",
    "        detector_backend='mtcnn'  # A robust face detector\n",
    "    )\n",
    "\n",
    "    # The result is a list of dictionaries, one for each face detected in the image.\n",
    "    # We'll assume there is only one face for this example.\n",
    "    if representations:\n",
    "        embedding = representations[0]['embedding']\n",
    "        print(\"Embedding shape:\", np.array(embedding).shape)\n",
    "        print(\"Embedding (first 10 values):\", embedding[:10])\n",
    "        print(\"Image path:\", img    )\n",
    "\n",
    "        # The size of the VGG-Face embedding is 2622\n",
    "        # This is a fixed-size vector representing the face.\n",
    "        print(\"Embedding size:\", len(embedding))\n",
    "    else:\n",
    "        print(\"No face detected in the image.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4f0005e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully with classes: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# set torch device to use gpu if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load dee-face-emotion-detector-classifier model\n",
    "model_path = 'emotion_detector_deepface.pth'\n",
    "if os.path.exists(model_path):\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model = EmotionClassifier(embedding_size=4096, num_classes=len(checkpoint['class_names']))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    class_names = checkpoint['class_names']\n",
    "    print(f\"Model loaded successfully with classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97e8a885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Predicted class for the image Dataset/images/validation/fear/21.jpg: fear\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "# valdate the model on a sample image\n",
    "img_path = 'Dataset/images/validation/fear/21.jpg'  # Example image path\n",
    "img = cv2.imread(img_path)\n",
    "if img is None:\n",
    "    print(\"Error: Image not found or could not be loaded.\")\n",
    "else:   \n",
    "    # Get the embedding using DeepFace.represent()\n",
    "    representations = DeepFace.represent(\n",
    "        img_path=img_path,\n",
    "        model_name='VGG-Face',\n",
    "        detector_backend='mtcnn'  # A robust face detector\n",
    "    )\n",
    "\n",
    "    if representations:\n",
    "        print(\"device:\", device)\n",
    "        model.to(device)\n",
    "        embedding = representations[0]['embedding']\n",
    "        embedding_tensor = torch.tensor(embedding, dtype=torch.float).unsqueeze(0).to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            output = model(embedding_tensor)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            predicted_class = class_names[predicted.item()]\n",
    "\n",
    "        print(f\"Predicted class for the image {img_path}: {predicted_class}\")\n",
    "    else:\n",
    "        print(\"No face detected in the image.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef015ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict emotion from an image\n",
    "\n",
    "def predict_emotion(image_path, model):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(\"Error: Image not found or could not be loaded.\")\n",
    "        return None\n",
    "\n",
    "    representations = DeepFace.represent(\n",
    "        img_path=image_path,\n",
    "        model_name='VGG-Face',\n",
    "        detector_backend='mtcnn'\n",
    "    )\n",
    "\n",
    "    if representations:\n",
    "        embedding = representations[0]['embedding']\n",
    "        embedding_tensor = torch.tensor(embedding, dtype=torch.float).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(embedding_tensor)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            predicted_class = class_names[predicted.item()]\n",
    "\n",
    "        return predicted_class\n",
    "    else:\n",
    "        print(\"No face detected in the image.\")\n",
    "        return None\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb2fd2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotion for the image Dataset/images/validation/fear/21.jpg:    fear\n",
      "Predicted emotion for the image Dataset/images/validation/happy/331.jpg:    happy\n",
      "Predicted emotion for the image Dataset/images/validation/sad/822.jpg:    neutral\n",
      "Predicted emotion for the image Dataset/images/validation/neutral/1045.jpg:    neutral\n",
      "Predicted emotion for the image Dataset/images/validation/surprise/3874.jpg:    surprise\n",
      "Predicted emotion for the image Dataset/images/validation/angry/1177.jpg:    angry\n"
     ]
    }
   ],
   "source": [
    "# call function to predict emotion from an image\n",
    "\n",
    "image_path = ['Dataset/images/validation/fear/21.jpg',\n",
    "              'Dataset/images/validation/happy/331.jpg',\n",
    "              'Dataset/images/validation/sad/822.jpg',\n",
    "              'Dataset/images/validation/neutral/1045.jpg',\n",
    "              'Dataset/images/validation/surprise/3874.jpg',\n",
    "              'Dataset/images/validation/angry/1177.jpg'] \n",
    "\n",
    "\n",
    "\n",
    "for image_path in image_path:\n",
    "    predicted_emotion = predict_emotion(image_path, model)\n",
    "    if predicted_emotion:\n",
    "        print(f\"Predicted emotion for the image {image_path}:    {predicted_emotion}\")\n",
    "    else:\n",
    "        print(\"Could not predict emotion for the image.\", image_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
